{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1655d7e",
   "metadata": {},
   "source": [
    "# Goodreads web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c941c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0fcf7ccf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# connect to website and pull in data\n",
    "\n",
    "URL = 'https://www.goodreads.com/list/show/1.Best_Books_Ever'\n",
    "\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\",  \"Accept-Encoding\": \"gzip, deflate, br\", \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",  \"Upgrade-Insecure-Requests\": \"1\"}\n",
    "\n",
    "page = requests.get(URL, headers = headers)\n",
    "\n",
    "soup1 = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "soup2 = BeautifulSoup(soup1.prettify(), \"html.parser\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b06652b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# extract title\n",
    "\n",
    "titles = soup2.find_all(\"a\", attrs={'class':'bookTitle'})\n",
    "\n",
    "titles_list = []\n",
    "\n",
    "for title in titles:\n",
    "    titles_list.append(title.text.strip())\n",
    "    \n",
    "#print(titles_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b34d0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "#extract book link\n",
    "\n",
    "links = soup2.find_all(\"a\", attrs={'class':'bookTitle'})\n",
    "\n",
    "links_list = []\n",
    "\n",
    "base_url = 'https://goodreads.com'\n",
    "\n",
    "for link in links:\n",
    "    links_list.append(base_url+link.get('href'))\n",
    "    \n",
    "#print(links_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ac65a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# extract author name\n",
    "\n",
    "authors = soup2.find_all(\"a\", attrs={'class':'authorName'})\n",
    "\n",
    "authors_list = []\n",
    "\n",
    "for author in authors:\n",
    "    authors_list.append(author.text.strip())\n",
    "    \n",
    "#print(authors_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c5094b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4.33', '4.50', '4.28', '4.26', '4.39', '3.65', '3.98', '4.61', '4.27', '4.14', '4.12', '4.31', '4.38', '3.89', '4.23', '4.15', '4.47', '3.93', '4.30', '4.32', '3.91', '4.06', '4.15', '4.26', '4.15', '4.20', '3.69', '3.74', '4.30', '4.07', '4.47', '4.31', '4.01', '4.20', '3.88', '4.23', '3.91', '3.80', '3.97', '3.99', '4.11', '4.43', '3.99', '4.27', '4.15', '4.44', '4.12', '3.98', '4.14', '4.19', '3.80', '4.34', '3.86', '4.62', '3.85', '3.83', '4.24', '4.12', '4.13', '4.31', '3.93', '3.87', '3.88', '4.26', '4.09', '4.11', '4.58', '4.05', '4.33', '4.35', '3.99', '4.33', '4.30', '4.08', '4.24', '3.79', '4.26', '4.16', '4.26', '4.39', '4.20', '4.10', '4.09', '4.36', '4.29', '4.00', '3.99', '4.28', '4.02', '4.11', '4.14', '4.06', '4.26', '4.25', '4.26', '4.38', '3.89', '4.02', '4.09', '3.80']\n"
     ]
    }
   ],
   "source": [
    "# extract average ratings\n",
    "\n",
    "ratings = soup2.find_all(\"span\", attrs={'class':'minirating'})\n",
    "\n",
    "ratings_list = []\n",
    "\n",
    "for rating in ratings:\n",
    "    ratings_list.append(rating.text.strip().replace('really liked it','').replace(' ','').replace('\\n','')[:4])\n",
    "    \n",
    "#print(ratings_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "606a42d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# extract score\n",
    "\n",
    "scores = soup2.find_all(\"a\", attrs={'onclick':\"Lightbox.showBoxByID('score_explanation', 300); return false;\"})\n",
    "\n",
    "scores_list = []\n",
    "\n",
    "for score in scores:\n",
    "    scores_list.append(score.text.strip()[7:])\n",
    "    \n",
    "print(scores_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "20edd7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create CSV and write headers and data into file\n",
    "\n",
    "goodreads_dict = {\n",
    "    'Title': titles_list,\n",
    "    'Link' : links_list, \n",
    "    'Author' : authors_list,\n",
    "    'Rating' : ratings_list,\n",
    "    'Score' : scores_list\n",
    "}\n",
    "\n",
    "goodreads_df = pd.DataFrame(goodreads_dict)\n",
    "\n",
    "goodreads_df.to_csv(\"Best book ever by Goodreads.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3be6ae0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to extract 10 pages (6 dec)\n",
    "\n",
    "titles_list = []\n",
    "links_list = []\n",
    "authors_list = []\n",
    "ratings_list = []\n",
    "scores_list = []\n",
    "\n",
    "\n",
    "for i in range (1,11):\n",
    "    url = \"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=\"+str(i)\n",
    "\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\",  \"Accept-Encoding\": \"gzip, deflate, br\", \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",  \"Upgrade-Insecure-Requests\": \"1\"}\n",
    "\n",
    "    page = requests.get(url, headers = headers)\n",
    "\n",
    "    soup1 = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "    soup2 = BeautifulSoup(soup1.prettify(), \"html.parser\")\n",
    "\n",
    "    #extract title\n",
    "    titles = soup2.find_all(\"a\", attrs={'class':'bookTitle'})\n",
    "\n",
    "    for title in titles:\n",
    "        titles_list.append(title.text.strip())\n",
    "    \n",
    "    #extract link\n",
    "    links = soup2.find_all(\"a\", attrs={'class':'bookTitle'})\n",
    "\n",
    "    base_url = 'https://goodreads.com'\n",
    "\n",
    "    for link in links:\n",
    "        links_list.append(base_url+link.get('href'))\n",
    "    \n",
    "    #extract author\n",
    "    authors = soup2.find_all(\"a\", attrs={'class':'authorName'})\n",
    "\n",
    "    for author in authors:\n",
    "        authors_list.append(author.text.strip())  \n",
    "    \n",
    "    #extract rating\n",
    "    \n",
    "    ratings = soup2.find_all(\"span\", attrs={'class':'minirating'})\n",
    "\n",
    "    for rating in ratings:\n",
    "        ratings_list.append(rating.text.strip().replace('really liked it','').replace(' ','').replace('\\n','')[:4])    \n",
    "    \n",
    "    #extract score\n",
    "    scores = soup2.find_all(\"a\", attrs={'onclick':\"Lightbox.showBoxByID('score_explanation', 300); return false;\"})\n",
    "\n",
    "    for score in scores:\n",
    "        scores_list.append(score.text.strip()[7:])\n",
    "\n",
    "df = pd.DataFrame({'Title': titles_list,'Link' : links_list, 'Author' : authors_list,'Rating' : ratings_list,'Score' : scores_list})\n",
    "\n",
    "df.to_csv(\"BestBookByGoodreads.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e102813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
